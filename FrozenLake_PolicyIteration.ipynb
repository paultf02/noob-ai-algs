{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrozenLake Policy Iteration\n",
    "\n",
    "This is the openai gym environment, FrozenLake in its 4x4 and 8x8, probabilistic and deterministic variants. Dynamic programming is used and the model gets a 100% solve rate in deterministic environments and roughly a 10% solve rate in probabilistic or \"slippery\" environments. \n",
    "\n",
    "**Dynamic programming** is a brute force method of solving Markov Decision Processes or MDPs. MDPs have the *Strong Markovian Property*; a future state is determined only by the current state and action and not previous states and actions.\n",
    "\n",
    "**An MDP is a tuple consisting of:**\n",
    "**S** - All states\n",
    "**A** - All actions that can be taken by an agent\n",
    "**r(s, a, s')** - *Reward model* giving expected reward for taking action **a** in state **s** and ending up in                     state **s'**\n",
    "**T(s'|s, a)** - *Transition probability matrix*; probability that taking action **a** in state **s** leads to state **s'**\n",
    "**γ** - *Discount factor*; Value between [0, 1] that represents relative importance between present and future               rewards\n",
    "        If **γ** = 1 future rewards are not weighted differently to present rewards\n",
    "        If **γ** = 0 future rewards are not taken into consideration at all\n",
    "        **γ** = 0.9 is usually a good starting point (Since this is a hyperparameter you will have to tune it)\n",
    "\n",
    "A policy **π(s) = a** is the action **a** an agent takes when in state **s**. Policies can be either stochastic (e.g epsilon-greedy policy) or deterministic (greedy policy); but here we will only use a deterministic/greedy policy.\n",
    "\n",
    "The value of a state, or state-value, is the cumulative reward an agent expects to receive when in a state and following a certain policy. If **γ** < 1 then this is the **discounted** cumulative reward.\n",
    "\n",
    "The Bellman equation is used to relate the value of the current state to the reward it gets, plus the value of the next state (for a given policy).\n",
    "\n",
    "The first question one might ask when thinking about this is \"how do we know what the value of the next state is?\" and this is where dynamic programming - an iterative algorithm- comes in. We start off with all values initialised to zero. Then as we go through states using the Bellman equation and recieve rewards, we start getting approximates for what the value function is. As we go through more iterations, the value function \"bootstraps\" itself and converges to its true value for the given policy.\n",
    "\n",
    "**V_π(s) = r(s,a,s') + γV_π(s')** <-- the Bellman equation, a state-value function for policy π\n",
    "\n",
    "For stochastic environments (like the slippery FrozenLake) we need to add the values for each possible next state\n",
    "weighted by that state's probaility of occuring -> **T(s'|s, a)**\n",
    "\n",
    "\n",
    "Once we have found the values for all states we will update the policy to choose actions that lead to states with the highest value - acting greedily. We do this by finding the action which has the highest action value\n",
    "    action value = \n",
    "\n",
    "The policy iteration algorithm goes as follows:\n",
    "1. start with a random policy and value function and set epsilon - threshold for convergence\n",
    "2. policy evaluation:\n",
    "    delta = 0\n",
    "    for every state:\n",
    "        old = V_π(s) \n",
    "        s' = take action π(s) in the environment\n",
    "        new = r(s,a,s') + γV_π(s')\n",
    "        V_π(s) = new\n",
    "        delta = max(delta, abs(new - old))\n",
    "        if delta < epsilon then GOTO 3\n",
    "3. policy improvement: \n",
    "    for every state\n",
    "        π(s) = choose action which leads to state with the highest value function #\"acting greedily\"\n",
    "    if policy has converged then STOP else GOTO 2\n",
    "\n",
    "NOTE: Here we are giving a very basic overview of the algorithm without going too in detail into the equations and derivation. If you want a deeper and formal understanding then it is strongly recommended to do extra reading. Resources given below.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Cannot re-register id: DeterministicFrozenLake-v0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4a21367e7ad1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'map_name'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'4x4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_slippery'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmax_episode_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mreward_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.78\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# optimum = .8196\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, id, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot re-register id: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: Cannot re-register id: DeterministicFrozenLake-v0"
     ]
    }
   ],
   "source": [
    "#registering environments that are deterministic based on the 4x4 and 8x8 grids\n",
    "#this code was copied from https://github.com/openai/gym/issues/565\n",
    "register(\n",
    "    id='DeterministicFrozenLake-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.78, # optimum = .8196\n",
    ")\n",
    "\n",
    "register(\n",
    "    id='DeterministicFrozenLake8x8-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '8x8', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.78, # optimum = .8196\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is used after training to see how our policy performs\n",
    "def execute(env, policy, render=False, num_tests=100):\n",
    "    \"\"\"env: openai gym environment\n",
    "       policy: dictionary, policy[state] is action for state that this policy takes \"\"\"\n",
    "    \n",
    "    episode_steps = []\n",
    "    num_completed = 0\n",
    "    for _ in range(num_tests):\n",
    "        steps = 0\n",
    "        state = env.reset() #starting state\n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(0.5) #gives a pause between each move for easy viewing\n",
    "            \n",
    "            #environment returns the state, reward, is the game finished, debugging info\n",
    "            #this is based on the action taken by the agent\n",
    "            #the action is determined by the policy for that state\n",
    "            action = policy[state]\n",
    "            state, reward, done, _ = env.step(action) \n",
    "            \n",
    "            steps += 1\n",
    "            if done: break\n",
    "        \n",
    "        #check if we reached the terminal(goal) state\n",
    "        if state == env.env.nS-1: #if reward == 1:       is an equivalent condition in this case \n",
    "            num_completed += 1\n",
    "            episode_steps.append(steps) # list of number of steps it took to finish game\n",
    "        \n",
    "    return episode_steps, num_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the algorithm doing all the work\n",
    "\n",
    "def policy_evaluation(env, policy, V, GAMMA=0.9):\n",
    "    epsilon = 1e-3 #the number the delta should be smaller than\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.env.nS-1):\n",
    "            old = V[s]\n",
    "            V[s] = 0\n",
    "            action = policy[s]\n",
    "            #going through all s' that (s, a) could lead to. For determninistic envirnoments there is only one s'\n",
    "            for (prob, next_state, reward, _) in env.env.P[s][action]: \n",
    "                V[s] += prob * (reward + GAMMA * V[next_state]) #bellman equation used to update value function\n",
    "            \n",
    "            #the most the value function was off compared to its previous value\n",
    "            #this will be a small number when converged\n",
    "            delta = max(delta, abs(V[s]-old)) \n",
    "        if delta < epsilon: break\n",
    "    return V\n",
    "\n",
    "def policy_improvement(env, policy, V, GAMMA=0.9):\n",
    "    stable = True\n",
    "    for s in range(env.env.nS-1):\n",
    "        old = policy[s]\n",
    "        best_value = float('-inf')\n",
    "        #choosing best action (action which leads to the state with the highest value)\n",
    "        for action in range(env.env.nA-1):\n",
    "            action_value = 0\n",
    "            #going through all s' that (s, a) could lead to. For determninistic envirnoments there is only one s'\n",
    "            for (prob, next_state, reward, _) in env.env.P[s][action]:\n",
    "                action_value += prob * (reward + GAMMA * V[next_state]) \n",
    "            if action_value > best_value:\n",
    "                best_value, best_action = action_value, action\n",
    "\n",
    "        if best_action != old:\n",
    "            stable = False\n",
    "            \n",
    "        policy[s] = best_action\n",
    "        \n",
    "    return policy, stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed the game 100 / 100 times\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"FrozenLake-v0\")\n",
    "    #env = gym.make(\"DeterministicFrozenLake-v0\")\n",
    "    #env = gym.make(\"FrozenLake8x8-v0\")\n",
    "    #env = gym.make(\"DeterministicFrozenLake8x8-v0\")\n",
    "    # the actions are ACTIONS = (0,1,2,3) LEFT, UP, RIGHT, DOWN\n",
    "    \n",
    "    RENDER = False #set to True if you want to see the game being played (reduce NUM_TESTS to ~3-5)\n",
    "    NUM_TESTS = 100 #number of game tests\n",
    "    NUM_EPOCHS = 1000 #number of training iterations\n",
    "    \n",
    "    #initialising value function V(s)\n",
    "    #NOTE: value \"function\" is actually a LIST of state-values\n",
    "    V = [0 for _ in range(env.env.nS)]\n",
    "    #V[env.env.nS-1] = 1\n",
    "    #policy initiated with random actions for each state \n",
    "    policy = {state: np.random.randint(0, env.env.nA) for state in range(env.env.nS)} \n",
    "    \n",
    "    #policy iteration algorithm: policy1-> valuefunction1-> policy2-> valuefunction2-> ...\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        \n",
    "        #_stable gives a definitive point to when the policy has converged EXCEPT in the \n",
    "        #case of two or more equally optimal actions. see if you can find a way around this\n",
    "        V = policy_evaluation(env, policy, V)\n",
    "        policy, _stable = policy_improvement(env, policy, V)\n",
    "        \n",
    "    #testing the policy\n",
    "    episode_steps, num_completed = execute(env, policy, render=RENDER, num_tests=NUM_TESTS)\n",
    "    \n",
    "    print(\"Completed the game\", num_completed, \"/\", NUM_TESTS, \"times\")\n",
    "    \n",
    "    #number of times it took n steps to complete\n",
    "    episode_steps = Counter(episode_steps) \n",
    "    \n",
    "    #plotting graph\n",
    "    heights = [episode_steps[n] for n in sorted(episode_steps.keys())]\n",
    "    plt.bar(range(len(episode_steps)), heights, align='center', width=0.8)\n",
    "    plt.xticks(range(len(episode_steps)), sorted(episode_steps.keys()))\n",
    "    plt.xlabel(\"Number of steps taken to complete game\")\n",
    "    plt.ylabel(\"Number of times it took x steps\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acknowledgements:\n",
    "Siraj Raval, Colin Skow and the rest of the School of AI team have provided excellent learning material in the form of their free course *Introduction to Reinforcment Learning*\n",
    "\n",
    "*Reinforcement Learning: An Introduction by Sutton and Barto* has been a tremendous resource\n",
    "\n",
    "These are some useful articles:\n",
    "https://towardsdatascience.com/reinforcement-learning-demystified-36c39c11ec14\n",
    "https://towardsdatascience.com/reinforcement-learning-demystified-solving-mdps-with-dynamic-programming-b52c8093c919\n",
    "https://stackoverflow.com/questions/37370015/what-is-the-difference-between-value-iteration-and-policy-iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
